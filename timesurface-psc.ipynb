{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import tables as tb\n",
    "import datetime\n",
    "import tonic\n",
    "import tonic.transforms as transforms\n",
    "import numpy as np\n",
    "import itertools\n",
    "from prosper.em import EM\n",
    "from prosper.utils import create_output_path\n",
    "from prosper.em.annealing import LinearAnnealing\n",
    "from prosper.em.camodels.bsc_et import BSC_ET\n",
    "from prosper.utils.datalog import dlog, StoreToH5, TextPrinter, StoreToTxt\n",
    "\n",
    "# parameters\n",
    "dataset_name = 'POKERDVS' # POKERDVS, NMNIST, NCARS or IBMGesture\n",
    "build_timesurfaces = True\n",
    "verbose = 1 # 0, 1, or 2 to print more information\n",
    "\n",
    "# initialisation\n",
    "save_location = Path('data/{}'.format(dataset_name.lower()))\n",
    "filename = '{}'.format(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "output_path = create_output_path(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building time surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if build_timesurfaces:\n",
    "    # time surface parameters\n",
    "    surface_dimensions = [11,11] # surface dimensions not necessarily square\n",
    "    time_constant = 20000 # decay time constant for time surfaces\n",
    "    download_dataset = True # downloads the datasets before parsing\n",
    "    decay='exp'\n",
    "    save_location = Path('data/{}'.format(dataset_name.lower()))\n",
    "\n",
    "    # initialising the time surface transform\n",
    "    transform = transforms.Compose([transforms.ToTimesurface(surface_dimensions=surface_dimensions, tau=time_constant, decay=decay, merge_polarities=True)])\n",
    "\n",
    "    # choosing event-based dataset\n",
    "    if dataset_name == 'NCARS': # 304 x 240\n",
    "        train_data = tonic.datasets.NCARS(save_to=str(save_location), train=True, download=download_dataset, transform=transform)\n",
    "        test_data = tonic.datasets.NCARS(save_to=str(save_location), train=False, download=download_dataset, transform=transform)\n",
    "    if dataset_name == 'POKERDVS': # 35 x 35\n",
    "        train_data = tonic.datasets.POKERDVS(save_to=str(save_location), train=True, download=download_dataset, transform=transform)\n",
    "        test_data = tonic.datasets.POKERDVS(save_to=str(save_location), train=False, download=download_dataset, transform=transform)\n",
    "    elif dataset_name == \"IBMGesture\": # 128 x 128\n",
    "        train_data = tonic.datasets.IBMGesture(save_to=str(save_location), train=True, download=download_dataset, transform=transform)\n",
    "        test_data = tonic.datasets.IBMGesture(save_to=str(save_location), train=False, download=download_dataset, transform=transform)\n",
    "    elif dataset_name == 'NMNIST': # 34 x 34\n",
    "        train_data = tonic.datasets.NMNIST(save_to=str(save_location), train=True, download=download_dataset, transform=transform)\n",
    "        test_data = tonic.datasets.NMNIST(save_to=str(save_location), train=False, download=download_dataset, transform=transform)    \n",
    "\n",
    "    train_loader = tonic.datasets.DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "    test_loader = tonic.datasets.DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "\n",
    "    # parsing training dataset\n",
    "    X_train, y_train, train_bin_counts   = [], [], []\n",
    "    for surfaces, target in iter(train_loader):\n",
    "        surfaces = surfaces.squeeze().numpy()\n",
    "        surfaces = surfaces.reshape(surfaces.shape[0], -1)\n",
    "        train_bin_counts.append(len(surfaces))\n",
    "        X_train.extend(surfaces)\n",
    "        y_train.extend(itertools.repeat(int(target),surfaces.shape[0]))\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    # parsing test dataset\n",
    "    X_test, y_test, test_bin_counts = [], [], []\n",
    "    for surfaces, target in iter(test_loader):\n",
    "        surfaces = surfaces.squeeze().numpy()\n",
    "        surfaces = surfaces.reshape(surfaces.shape[0], -1)\n",
    "        test_bin_counts.append(len(surfaces))\n",
    "        X_test.extend(surfaces)\n",
    "        y_test.extend(itertools.repeat(int(target),surfaces.shape[0]))\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    # save data\n",
    "    np.savez(save_location/'timesurfaces.npz',\n",
    "             X_train=X_train,\n",
    "             X_test=X_test,\n",
    "             y_train=y_train,\n",
    "             y_test=y_test,\n",
    "             train_bin_counts=train_bin_counts,\n",
    "             test_bin_counts=test_bin_counts)\n",
    "    \n",
    "    timesurfaces = {'X_train':X_train,\n",
    "                    'X_test':X_test,\n",
    "                    'y_train':y_train,\n",
    "                    'y_test':y_test,\n",
    "                    'train_bin_counts':train_bin_counts,\n",
    "                    'test_bin_counts':test_bin_counts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training sparse coding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading time surfaces when build_timesurfaces = False\n",
    "if not build_timesurfaces:\n",
    "    try:\n",
    "        timesurfaces = np.load(save_location/'timesurfaces.npz')\n",
    "    except:\n",
    "        print('Generate and save some time surfaces first.')\n",
    "\n",
    "H = 400 # Dimensionality of the model\n",
    "D = timesurfaces['X_train'].shape[1] # dimensionality of observed data\n",
    "\n",
    "# Approximation parameters for Expectation Truncation (It has to be Hprime>=gamma)\n",
    "Hprime = 5\n",
    "gamma = 3\n",
    "\n",
    "# Import and instantiate a model\n",
    "model = BSC_ET(D, H, Hprime, gamma)\n",
    "\n",
    "# Configure DataLogger    \n",
    "h5_list = ('T', # temperature for deterministic annealing (avoids local optima)\n",
    "           'L', # log likelihood\n",
    "           'pi', # prior\n",
    "           'sigma', # standard deviation\n",
    "           'W') # mean\n",
    "\n",
    "# Stores things in an h5 file\n",
    "dlog.set_handler(h5_list, StoreToH5, output_path+'/results.h5')\n",
    "\n",
    "# print to terminal\n",
    "if verbose > 1:\n",
    "    print_list = ('T', 'L', 'pi', 'sigma')\n",
    "    dlog.set_handler(print_list, TextPrinter)\n",
    "\n",
    "# Choose annealing schedule\n",
    "anneal = LinearAnnealing(20)\n",
    "anneal['T'] = [(0, 5.), (.8, 1.)]\n",
    "anneal['Ncut_factor'] = [(0, 0.), (0.5, 0.), (0.6, 1.)]\n",
    "anneal['W_noise'] = [(0, np.std(timesurfaces['X_train']) / 2.), (0.7, 0.)]\n",
    "anneal['anneal_prior'] = False\n",
    "\n",
    "# initialise model parameters\n",
    "training_dict = {'y': timesurfaces['X_train'], 'l': timesurfaces['y_train']}\n",
    "model_params = model.standard_init(training_dict)\n",
    "if verbose > 0:\n",
    "    print(\"model defined\")\n",
    "\n",
    "em = EM(model=model, anneal=anneal)\n",
    "em.data = training_dict\n",
    "em.lparams = model_params\n",
    "em.run()\n",
    "if verbose > 0:\n",
    "    print(\"em finished\")\n",
    "    \n",
    "dlog.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on training set\n",
    "results_train = model.inference(anneal, em.lparams, training_dict)\n",
    "\n",
    "start = 0\n",
    "train_features, train_labels = [], []\n",
    "for i in range(len(timesurfaces['train_bin_counts'])):\n",
    "    stop = start + timesurfaces['train_bin_counts'][i]\n",
    "    train_features.append(results_train['s'][start:stop, 0, :].mean(0))\n",
    "    this_l = timesurfaces['y_train'][start:stop]\n",
    "    assert (this_l == this_l[0]).all()\n",
    "    train_labels.append(this_l[0])\n",
    "    start = stop\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# inference on test set\n",
    "test_dict = {'y': timesurfaces['X_test']}\n",
    "results_test = model.inference(anneal, em.lparams, test_dict)\n",
    "\n",
    "start = 0\n",
    "test_features, test_labels = [], []\n",
    "for i in range(len(timesurfaces['test_bin_counts'])):\n",
    "    stop = start + timesurfaces['test_bin_counts'][i]\n",
    "    test_features.append(results_test['s'][start:stop, 0, :].mean(0))\n",
    "    this_l = timesurfaces['y_test'][start:stop]\n",
    "    \n",
    "    assert (this_l == this_l[0]).all()\n",
    "    test_labels.append(this_l[0])\n",
    "    start = stop\n",
    "    \n",
    "test_features = np.array(test_features)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs', multi_class='auto',max_iter=1000)\n",
    "logreg.fit(train_features,train_labels)\n",
    "score = logreg.score(test_features, test_labels)\n",
    "\n",
    "# print mean score\n",
    "print('Logistic Regression Classifier: %s' % score)\n",
    "np.save(output_path+'/classification.npy', score)\n",
    "\n",
    "if verbose > 1:\n",
    "    lr_tr_pred = logreg.predict(train_features)\n",
    "    lr_te_pred = logreg.predict(test_features)\n",
    "\n",
    "    print(\"Training dataset results for classifier: \\n\\n%s\\n\\n%s\\n\" % (logreg, metrics.classification_report(train_labels, lr_tr_pred)))\n",
    "    print(\"Confusion matrix:\\n\\n%s\\n\"  % metrics.confusion_matrix(train_labels, lr_tr_pred))\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    print(\"\\nTest dataset results for classifier: \\n\\n%s\\n\\n%s\\n\" % (logreg, metrics.classification_report(test_labels, lr_te_pred)))\n",
    "    print(\"Confusion matrix:\\n\\n%s\\n\" % metrics.confusion_matrix(test_labels, lr_te_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import seaborn as sns\n",
    "\n",
    "file=tb.open_file(output_path+\"results.h5\", mode='r')\n",
    "\n",
    "# read parameters\n",
    "Ws=file.root.W.read()\n",
    "pi=file.root.pi.read()\n",
    "sigma=file.root.sigma.read()\n",
    "\n",
    "# sigma plot\n",
    "plt.figure()\n",
    "plt.plot(sigma)\n",
    "locator = matplotlib.ticker.MultipleLocator(2)\n",
    "plt.gca().xaxis.set_major_locator(locator)\n",
    "formatter = matplotlib.ticker.StrMethodFormatter(\"{x:.0f}\")\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "plt.show()\n",
    "plt.savefig(output_path+'sigma.pdf')\n",
    "\n",
    "# pi plot\n",
    "fig = plt.figure(2)\n",
    "plt.plot(pi*100)\n",
    "locator = matplotlib.ticker.MultipleLocator(2)\n",
    "plt.gca().xaxis.set_major_locator(locator)\n",
    "formatter = matplotlib.ticker.StrMethodFormatter(\"{x:.0f}\")\n",
    "plt.gca().xaxis.set_major_formatter(formatter)\n",
    "plt.show()\n",
    "plt.savefig(output_path+'pi.pdf')\n",
    "\n",
    "# dictionary of features\n",
    "for ind in [0,1,2,5,10,-1]:\n",
    "    print(\"iteration\", Ws.shape[0]-1) if ind == -1 else print(\"iteration\", ind)\n",
    "    fig=plt.figure(figsize=(10,10))\n",
    "    rows=np.ceil(np.sqrt(Ws.shape[-1]))\n",
    "    cols=np.ceil(np.sqrt(Ws.shape[-1]))\n",
    "    D=int(np.sqrt(Ws.shape[-2]))\n",
    "    \n",
    "    for i in range(Ws.shape[-1]):    \n",
    "        ax= fig.add_subplot(rows,cols,i+1)\n",
    "        ax.imshow(Ws[ind,:,i].reshape((D,D)),interpolation='nearest')\n",
    "        ax.axis('off')\n",
    "    plt.savefig(output_path+'dictionary_{}.pdf'.format(ind))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
